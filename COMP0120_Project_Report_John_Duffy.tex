\documentclass[10pt, a4paper,reqno]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\title{COMP0120 Project - Support Vector Machines (SVM\MakeLowercase{s})}
\author{\textbf{John Duffy, Student Number: 19154676}}

\begin{document}

\maketitle

\section{Introduction}

Suppose we have been provided with a dataset $L$ consisting of $N$ elements, each with a $x_0$ and $x_1$ value. Furthermore, suppose we have been provided with a vector $y$ of length $N$ consisting of the values $-1$ and $+1$, where each index of $y$ corresponds to the same index of $L$. So, each element of $L$ has an associated value $-1$ or $+1$.

We could plot the elements of $L$ as per Figure 1. Clearly, the elements with the value $-1$ are clustered together, as are those with the value $+1$, and without overlap. The elements are $linearly$  $separable$.

Now suppose we wish to draw a line $l$ between the two clusters, such that additional elements without a known value can be accurately classified as being of value $-1$ or $+1$. This process is called \emph{linear binary classification}; \emph{linear} because we wish to separate the classes (the clusters of values $-1$ and $+1$) with a line, and \emph{binary} because we have two classes. This process can be extended to be $non-linear$, i.e. use a higher degree polynomial to separate the classes, and to include additional classes.  

Implementing this process using a method called Support Vector Machines (SVMs), first conceived of by Cortes and Vapnik [1], is the subject of this report.

\begin{figure}[H]
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_0.pdf}
	\caption{Dataset $L$}
\end{figure}


%
% Section
%
\clearpage\section{Mathematical Setting}

This section provides a summary of the mathematical setting for SVMs, and is based on \emph{Support Vector Machines Explained} by Tristan Fletcher [2], \emph{CS229 Lecture Notes, Support Vector Machines} by Andrew Ng [3], Aur\'{e}lien G\'{e}ron's \emph{Hands on Machine Learning with Scikit-Learn, Keras \& TensorFlow}[4], R. Berwick's \emph{An Idiot's Guide to Support Vector Machines}[5].

\subsection{Linear Binary Classification}

Consider our dataset $L$ with $N$ elements, with each element having two \emph{features}, $x_0$ and $x_1$, i.e. of dimensionality $D = 2$, and being in either Class $-1$ or Class $+1$:

\begin{equation}
\{\mathbf{x}_i, y_i\}\text{ where }i = 1...L,\text{ }\mathbf{x} \in \mathbb{R}^{D},\text{ }y_i \in \{-1, +1\}
\end{equation}

\begin{figure}[H]
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_1.pdf}
	\caption{Dataset $L$}
\end{figure}

How should we decide where to draw the line $l$? Or, when $D > 2$, the hyperplane between the classes? Consider Figure 2. If we used the line $l_1$ then additional elements of Class $-1$ may be incorrectly classified as Class $+1$. Conversely, if we used the line $l_2$ then additional elements of Class $+1$ may be incorrectly classified as Class $-1$.

The line/hyperplane between the classes can be described by:

\begin{equation}
\mathbf{w}\cdot\mathbf{x} + b = 0
\end{equation}

where $\mathbf{w}$ is normal to the hyperplane and $\frac{b}{||\mathbf{w}||}$ is the perpendicular distance to the origin.

\emph{Support Vectors} are the elements of $L$ which are closest to the hyperplane, and SVMs orientate the hyperplane to be a maximum distance from the \emph{Support Vectors}. This maximises the width of the ``street" between classes, which is depicted in Figure 3.

\begin{figure}
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_2.pdf}
	\caption{SVM linear separation of Class $-1$ and Class $+1$ by maximising the width of the``street" between the classes.}
\end{figure}

To implement a SVM we select the variables $\mathbf{w}$ and $b$ such that our dataset $L$ can be described as:

\begin{equation}
\mathbf{x}_i\cdot\mathbf{w} + b \geq +1,\text{ for }y_i = +1
\end{equation}

\begin{equation}
\mathbf{x}_i\cdot\mathbf{w} + b \leq -1,\text{ for }y_i = -1
\end{equation}

which when combined yields:

\begin{equation}
y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1 \geq 0\text{ }\forall{i}
\end{equation}

To orientate the hyperplane to be as far away as possible from the \emph{Support Vectors} we need to maximise the \emph{margin}, depicted by $d_1$ and $d_2$ on Figure 3.

By geometry, the \emph{margin} is $\frac{1}{||\mathbf{w}||}$. So, maximising the \emph{margin} is equivalent to minimising $||\mathbf{w}||$, which is equivalent to minimising $\frac{1}{2}||\mathbf{w}||^2$. This is convenient from an optimisation formulation perspective.

So, we can formulate the problem of finding $\mathbf{w}$ and $b$ as the optimisation problem:

\begin{equation}
min\frac{1}{2}||\mathbf{w}||^2\text{, such that }y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1 \geq 0\text{ }\forall{i}
\end{equation}

This is an optimisation problem with a convex quadratic objective function and linear constraints.

\subsubsection{Primal Problem}

The Lagrangian of the optimisation problem includes in equation (6) each constraint with it's associated multiplier ($\alpha_i \geq 0, \forall i$):

\begin{equation}
min\mathcal{L}_P(\mathbf{w}, b, \alpha) = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^{L}\alpha_i[y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1]
\end{equation}

\begin{equation}
min\mathcal{L}_P(\mathbf{w}, b, \alpha) = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^{L}\alpha_i y_i(\mathbf{x}_i\cdot\mathbf{w} + b) + \sum_{i=1}^{L}\alpha_i
\end{equation}

Setting the derivatives of $\mathcal{L}_P$ with respect to $\mathbf{w}$ and $b$ to zero:

\begin{equation}
\frac{\partial \mathcal{L}_P}{\partial \mathbf{w}} = \mathbf{w} -  \sum_{i=1}^{L}\alpha_i y_i \mathbf{x}_i = 0
\end{equation}
 
\begin{equation}
\frac{\partial \mathcal{L}_P}{\partial b} = \sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

which yields:

\begin{equation}
\mathbf{w} = \sum_{i=1}^{L}\alpha_i y_i \mathbf{x}_i
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

The Lagrangian is called the \emph{primal} form of the optimisation problem, or \emph{primal problem}, indicated by the $P$ subscript, $\mathcal{L}_P$.

The intercept \emph{b} can be determined as follows[2]:

\begin{equation}
b = \frac{1}{N_S}\sum_{s\in S}(y_s - \sum_{m\in S}\alpha_m y_m \mathbf{x}_m\cdot\mathbf{x}_s)
\end{equation}

where S is the set of \emph{Support Vectors}.

Subsequent classification of new data $\mathbf{x}^*$is achieved as follows[2]:

\begin{equation}
y^* = sgn(\mathbf{w}\cdot\mathbf{x}^* + b)
\end{equation}


\subsubsection{Dual Problem}

Whereas the \emph{primal problem} seeks to minimises $\mathcal{L}_P(\mathbf{w}, b, \alpha)$, the \emph{dual problem} seeks to maximise $\mathcal{L}_D(\alpha)$, where $\alpha$ is the $\emph{dual}$ variable[5], using the previously determined relations of equations (11) and (12).

Substituting (11) and (12) into (8) yields:

\begin{equation}
max\mathcal{L}_D(\alpha) = \sum_{i=1}^{L}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j - b\sum_{i=1}^{L}\alpha_i y_i
\end{equation}

From (12), the final term is zero, so the \emph{dual problem} simplifies to:

\begin{equation}
max\mathcal{L}_D(\alpha) = \sum_{i=1}^{L}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j
\end{equation}

such that:

\begin{equation}
\alpha_i\geq0\text{ }\forall{i}
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

So, what have we achieved in going from the \emph{primal problem} to the \emph{dual problem}? We have removed the dependency on $\mathbf{w}$ and $b$ from $\mathcal{L}_D(\alpha)$. We can now maximise $\mathcal{L}_D(\alpha)$ with respect to $\alpha$, subject to the constraints, and then plug the resultant $\alpha$ into equation (11) to find $\mathbf{w}$. And subsequenty find $b$ from equation (13).

Importantly, the \emph{dual problem} is only dependent upon $y_i$ and the dot products $\mathbf{x}_i\cdot\mathbf{x}_j$. This can be exploited when considering non-linear classification.

\subsubsection{Hard Margin}

\subsubsection{Soft Margin}

\subsection{Non-Linear Classification}


%
% Section
%
\clearpage\section{Simulation Study - Linear Binary Classification}

The aim of this simulation study is to gain a deeper understanding of the underlying mathematics, and also a deeper intuitive understanding, of Linear Binary Classification SMVs by considering the simplest possible SVM.

\subsection{Chosen Data Set}

Rather than choosing a large dataset for this project, I decided to do the exact opposite and choose the smallest possible data set necessary to demonstrate the operation of SVMs. The reason for this is because it is easier to gain an understanding of the underlying mathematics, and also an intuitive understanding, when systems are reduced to the bare minimum functionality.

In the case of SVMs, starting with only two data points it is possible to demonstrate the solution of the \emph{dual problem} in $\mathcal{R}^2$ and visualise this on a 2D plot. From this point $\mathbf{w}$ and $b$ are easily determined which can then be used to visualise the \emph{decision boundary} and \emph{margin} on a 2D plot. And then using the \emph{classification function} it is easy to demonstrate the classification of new data points.


\subsection{Resulting Optimisation Problem}

The \emph{Dual Problem}:

\begin{equation}
max\mathcal{L}_D(\alpha) = \sum_{i=1}^{L}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j
\end{equation}

\begin{equation}
\alpha_i\geq0\text{ }\forall{i}
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

can be re-stated as the \emph{minimisation} problem:

\begin{equation}
min\mathcal{L}_D(\alpha) = \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j - \sum_{i=1}^{L}\alpha_i
\end{equation}

\begin{equation}
\alpha_i\geq0\text{ }\forall{i}
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

This can be re-stated as[2]:
\begin{equation}
min\mathcal{L}_D(\alpha) = \frac{1}{2}\alpha^T\mathbf{H}\alpha - \sum_{i=1}^{L}\alpha_i
\end{equation}

where $H_ij = y_i y_j \mathbf{x}_i\cdot\mathbf{x}_j$, subject to the same constraints:

\begin{equation}
\alpha_i\geq0\text{ }\forall{i}
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

For this \emph{Dual Problem} in $\mathcal{R}^2$, the equality constraint can be re-stated as:

\begin{equation}
\alpha_1 y_i + \alpha_2 y_2 = 0
\end{equation}

\begin{equation}
-\alpha_1 + \alpha_2 = 0\text{, or }\alpha_1 = \alpha_2
\end{equation}

Now, consider the data points $P_1 = [3.0, 3.0]^T \in \{Class -1\}$, and $P_2 = [6.0, 6.0]^T \in \{Class +1\}$. These data points and the resultant $\mathcal{L}_D(\alpha)$, having used these data points $\mathbf{w}$ and their classifications $y$ in determining $\mathbf{H}$, are depicted in Figure 4 together with the constraints.

Likewise, consider the data points $[3.0, 6.0]^T \in \{Class -1\}$, and $[6.0, 3.0]^T \in \{Class +1\}$. These data points and the resultant $\mathcal{L}_D(\alpha)$, having used these data points $\mathbf{w}$ and their classifications $y$ in determining $\mathbf{H}$, are depicted in Figure 6, together with the constraints.

In both cases, we have a \emph{convex} minimisation problem with one \emph{equality} constraint and two \emph{inequality} constraints. The solution to each minimisation problem is a vector $[\alpha_1, \alpha_2]^T$ which is used to determine $\mathbf{w}$ and $b$ in each case.


\subsection{Solution of Optimisation Problem}

Clearly, in each case, provided $P_1 \neq P_2$, then there is always a vector $P_1 - P_2$ which can be bisected, and an orthogonal vector to this can be used as the \emph{decision boundary}. In this simplistic case, the mathematics of SVMs are not required to determine the \emph{decision boundary}. 

However, in the SVM sense, provided $P_1 \neq P_2$, then $P_1$ and $P_2$ are \emph{Support Vectors} and therefore the solution $[\alpha_1, \alpha_2]^T$ to the minimisation problem cannot be $[0, 0]^T$. Therefore, for this simplified $\mathcal{L}_D(\alpha)$ minimisation problem we only need to consider the single equality constraint. The problem can be minimised using the \emph{Quadratic Penalty} method.

Consider the previously stated \emph{Dual Problem}:

\begin{equation}
min\mathcal{L}_D(\alpha) = \frac{1}{2}\alpha^T\mathbf{H}\alpha - \sum_{i=1}^{L}\alpha_i
\end{equation}

Including the equality constraint as a penalty factor, yields the following:

\begin{equation}
min\mathcal{L}_D(\alpha) = \frac{1}{2}\alpha^T\mathbf{H}\alpha - \sum_{i=1}^{L}\alpha_i - \frac{\mu}{2}(y\cdot\alpha)^2
\end{equation}

Minimising $\mathcal{L}_D(\alpha)$ using \emph{Steepest Descent} with \emph{backtracking}, with parameters of \emph{rho = 0.1}, $\mu = 500$ and $x_0 = [1, 1]^T$, and using equations (11) and (13), produces the following results, which are also depicted in Figures 5 and 7.

For $P_1 = [3.0, 3.0], P_2 = [6.0, 6.0]^T$:

\begin{verbatim}

Itertions: 115
Alphas: [0.12311629 0.11711603]
Support Vectors: [[3. 3.][6. 6.]]
w: [0.33334727 0.33334727]
b: -3.0001254473834473
Margin: 2.121231642573227

\end{verbatim}

For $P_1 = [3.0, 6.0], P_2 = [6.0, 3.0]^T$:

\begin{verbatim}

Itertions: 8
Alphas: [0.11111112 0.11111112]
Support Vectors: [[3. 6.][6. 3.]]
w: [0.33333336 -0.33333336]
b: 0.0
Margin: 2.121320173854028

\end{verbatim}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_data_0.pdf}
		\caption{$P_1 = [3.0, 3.0]^T, P_2 = [6.0, 6.0]^T$}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_0.pdf}
		\caption{Resultant $\mathcal{L}_D(\alpha)$ and constraints.}
	\end{subfigure}
	\caption{$P_1$ and $P_2$, together with their classification $y$, are used to determine the \emph{convex} minimisation problem $\mathcal{L}_D(\alpha)$ using equation (25).}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_solv_0.pdf}
		\caption{Quadratic Penalty Minimisation - Iterations: 115 Alphas: $[0.12311629 0.11711603]^T$.}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_0.pdf}
		\caption{Subsequent classification of 2000 random data points.}
	\end{subfigure}
	\caption{Substituting $[\alpha_1, \alpha_2]^T$ from the Quadratic Penalty minimisation into equation (11) yields $\mathbf{w}$:
 [0.33334727 0.33334727], from which the Margin: 2.121231642573227 can be determined. The value of b: -3.0001254473834473 is determined from equation (13). The subsequent classification of new random data is determined using equation (14).}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_data_1.pdf}
		\caption{$P_1 = [3.0, 6.0]^T, P_2 = [6.0, 3.0]^T$}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_1.pdf}
		\caption{Resultant $\mathcal{L}_D(\alpha)$ and constraints.}
	\end{subfigure}
	\caption{$P_1$ and $P_2$, together with their classification $y$, are used to determine the \emph{convex} minimisation problem $\mathcal{L}_D(\alpha)$ using equation (25).}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_solv_1.pdf}
		\caption{Quadratic Penalty Minimisation - Iterations: 8 Alphas: $[0.11111112 0.11111112]^T$.}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_1.pdf}
		\caption{Subsequent classification of 2000 random data points.}
	\end{subfigure}
	\caption{Substituting $[\alpha_1, \alpha_2]^T$ from the Quadratic Penalty minimisation into equation (11) yields $\mathbf{w}$:
 [0.33333336 -0.33333336], from which the Margin: 2.121320173854028 can be determined. The value of b: 0.0 is determined from equation (13). The subsequent classification of new random data is determined using equation (14).}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_2.pdf}
		\caption{Iterations: 1253 Alphas: [0.04104399 0.04104399 0.04104399 0.11713205] Support Vectors: [[2. 4.][3. 3.][4. 2.][6. 6.]] w: [0.3333964 0.3333964] b: -3.000473004866972 Margin: 2.1209190615442166}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_3.pdf}
		\caption{Itertions: 1522 Alphas: [0.11111534 0.03697217 0.037038   0.03710384] Support Vectors: [[3. 6.][5. 2.][6. 3.][7. 4.]] w: [0.33346966 -0.33321838] b: -0.0011467909155819522 Margin: 2.1212521606593167}
	\end{subfigure}
	\caption{Quadratic Penalty method minimisation of $\mathcal{L}_D(\alpha)$, and subsequent classification of 2000 new random data points.}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_0.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_1.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_2.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_3.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_4.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_5.pdf}
		\caption{Random $P_1, P_2$}
	\end{subfigure}
	\caption{Quadratic Penalty method minimisation of $\mathcal{L}_D(\alpha)$ generated from two randomly generated data points $P_1$ and $P_2$, and the subsequent classification of 2000 new random data points.}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_0.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_1.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_2.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_3.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_4.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_5.pdf}
		\caption{Random $P_1, P_2, P_3$}
	\end{subfigure}
	\caption{Quadratic Penalty method minimisation of $\mathcal{L}_D(\alpha)$ generated from three randomly generated data points $P_1$, $P_2$ and $P_3$, and the subsequent classification of 2000 new random data points.}
\end{figure}


\clearpage\section{Simulation Study - Non-Linear Binary Classification}


\subsection{Kernels}


\subsection{The Kernel Trick}


\subsection{Chosen Data Set}


\subsection{Resulting Optimisation Problem}


\subsection{Solution of Optimisation Problem}


\subsection{Algorithm 1}


\subsection{Algorithm 2}


\subsection{What about $\mathbf{w}$ and $b$?}


\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM - Randomly generated 4 points}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM - Randomly generated 8 points}
\end{figure}


%
% Section
%
\clearpage\section{Notes on Project Code}

The code for this project has been implemented in Python. For clarity, particularly for those readers with a MatLab background, is it worth highlighting noteworthy aspects of the code.

\subsection{Lambda Expressions}
In Python, like many \emph{functional} programming languages, functions are \emph{first-class citizens}. This means functions can be passed to/from other functions in the same manner as variables. This is similar to MatLab function handles. It is sometimes convenient, and clearer, to write a short functions as \emph{lambda} expressions, or \emph{anonymous functions}, rather that a write a full function declarations. A \emph{lambda} can also be assigned to a variable.

For example, the following function:

\begin{lstlisting}[language=Python]
	def linear_kernel(xi, xj):
		# The @ operator is the matrix multiplication
		# and dot product operator.
    		return xi @ xj
\end{lstlisting}

can be expressed as the following \emph{lambda} expression:

\begin{lstlisting}[language=Python]
	linear_kernel = lambda xi, xj: xi @ xj
\end{lstlisting}

An example of using a \emph{lambda} expression anonymously, passing a kernel function as a parameter, is:

\begin{lstlisting}[language=Python]
	linear_svm.train(X, y, lambda xi, xj: xi @ xj)
\end{lstlisting}

This functionality has been used throughout the project code for clarity and brevity.

For example, the \emph{dual problem} equation (as a minimisation):

\begin{equation}
\mathcal{L}_D(\alpha) = \frac{1}{2}\alpha^T\mathbf{H}\alpha - \sum_{i=1}^{L}\alpha_i
\end{equation}

is coded as follows (together with the first and second derivatives):

\begin{lstlisting}[language=Python]
    f   = lambda x: 0.5 * x @ H @ x - x @ (x / x)
    df  = lambda x: H @ x - x / x
    d2f = lambda x: H 
\end{lstlisting}

These are dimensionless expressions, which means that a vector $\mathbf{x}$ of any size can be used in the expression.

\subsection{Reimplementation of MatLab Functions}

A number of MatLab functions used in previous COMP0120 assignments have been reimplemented in Python, specifically $backtracking()$, $steepest\_descent()$ and $newton()$. The functionality of these functions was tested against the \emph{Rosenbrock} function and the results compared to the \emph{MatLab} results from Assignment 1.

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{rosenbrock_0.pdf}
		\caption{$x0 = [1.2, 1.2]^T$}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{rosenbrock_1.pdf}
		\caption{$x0 = [-1.2, 1.0]^T$}
	\end{subfigure}
	\caption{Functionality test of $backtracking()$, $steepest\_descent()$ and $newton()$ functions implemented in Python. With a value of $rho = 0.1$, both $steepest\_descent()$ and $newton()$ converge to $[1.0, 1.0]^T$ in the same number of iterations, and taking the same paths, as their \emph{MatLab} equivalent functions.}
\end{figure}


%
% Section
%
\clearpage\section{References}

[1] C. Cortes, V. Vapnik, in Machine Learning, pp. 273-297 (1995).

[2] T. Fletcher, Support Vector Machines Explained, March 1, 2009.

[3] A. Ng, CS229 Lecture Notes Part V, Support Vector Machines.

[4] A. G\'{e}ron, Hands-On Machine Learning with Scikit-Learn, Keras \& TensorFlow, Second Edition.

[5] R. Berwick, An Idiot's Guide to Support Vector Machines.

[6] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.



\end{document}


