\documentclass[10pt, a4paper,reqno]{amsart}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\title{COMP0120 Project - Support Vector Machines (SVM\MakeLowercase{s})}
\author{\textbf{John Duffy, Student Number: 19154676}}

\begin{document}

\maketitle

\section{Introduction}

Suppose we have been provided with a dataset $L$ consisting of $N$ elements, each with a $x_0$ and $x_1$ value. Furthermore, suppose we have been provided with a vector $y$ of length $N$ consisting of the values $-1$ and $+1$, where each index of $y$ corresponds to the same index of $L$. So, each element of $L$ has an associated value $-1$ or $+1$.

We could plot the elements of $L$ as per Figure 1. Clearly, the elements with the value $-1$ are clustered together, as are those with the value $+1$, and without overlap. The elements are $linearly$  $separable$.

Now suppose we wish to draw a line $l$ between the two clusters, such that additional elements without a known value can be accurately classified as being of value $-1$ or $+1$. This process is called \emph{linear binary classification}; \emph{linear} because we wish to separate the classes (the clusters of values $-1$ and $+1$) with a line, and \emph{binary} because we have two classes. This process can be extended to be $non-linear$, i.e. use a higher degree polynomial to separate the classes, and to include additional classes.  

Implementing this process using a method called Support Vector Machines (SVMs), first conceived of by Cortes and Vapnik [1], is the subject of this report.

\begin{figure}[H]
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_0.pdf}
	\caption{Dataset $L$}
\end{figure}

\section{Mathematical Setting}

This section provides a summary of the mathematical setting for SVMs, and is based on \emph{Support Vector Machines Explained} by Tristan Fletcher [2], \emph{CS229 Lecture Notes, Support Vector Machines} by Andrew Ng [3], Aur\'{e}lien G\'{e}ron's \emph{Hands on Machine Learning with Scikit-Learn, Keras \& TensorFlow}[4], R. Berwick's \emph{An Idiot's Guide to Support Vector Machines}[5].

\subsection{Linear Binary Classification}

Consider our dataset $L$ with $N$ elements, with each element having two \emph{features}, $x_0$ and $x_1$, i.e. of dimensionality $D = 2$, and being in either Class $-1$ or Class $+1$:

\begin{equation}
\{\mathbf{x}_i, y_i\}\text{ where }i = 1...L,\text{ }\mathbf{x} \in \mathbb{R}^{D},\text{ }y_i \in \{-1, +1\}
\end{equation}

\begin{figure}[H]
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_1.pdf}
	\caption{Dataset $L$}
\end{figure}

How should we decide where to draw the line $l$? Or, when $D > 2$, the hyperplane between the classes? Consider Figure 2. If we used the line $l_1$ then additional elements of Class $-1$ may be incorrectly classified as Class $+1$. Conversely, if we used the line $l_2$ then additional elements of Class $+1$ may be incorrectly classified as Class $-1$.

The line/hyperplane between the classes can be described by:

\begin{equation}
\mathbf{w}\cdot\mathbf{x} + b = 0
\end{equation}

where $\mathbf{w}$ is normal to the hyperplane and $\frac{b}{||\mathbf{w}||}$ is the perpendicular distance to the origin.

\emph{Support Vectors} are the elements of $L$ which are closest to the hyperplane, and SVMs orientate the hyperplane to be a maximum distance from the \emph{Support Vectors}. This maximises the width of the ``street" between classes, which is depicted in Figure 3.

\begin{figure}
	\centering	
	\includegraphics[width=1.0\textwidth]{intro_2.pdf}
	\caption{SVM linear separation of Class $-1$ and Class $+1$ by maximising the width of the``street" between the classes.}
\end{figure}

To implement a SVM we select the variables $\mathbf{w}$ and $b$ such that our dataset $L$ can be described as:

\begin{equation}
\mathbf{x}_i\cdot\mathbf{w} + b \geq +1,\text{ for }y_i = +1
\end{equation}

\begin{equation}
\mathbf{x}_i\cdot\mathbf{w} + b \leq -1,\text{ for }y_i = -1
\end{equation}

which when combined yields:

\begin{equation}
y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1 \geq 0\text{ }\forall{i}
\end{equation}

To orientate the hyperplane to be as far away as possible from the \emph{Support Vectors} we need to maximise the \emph{margin}, depicted by $d_1$ and $d_2$ on Figure 3.

By geometry, the \emph{margin} is $\frac{1}{||\mathbf{w}||}$. So, maximising the \emph{margin} is equivalent to minimising $||\mathbf{w}||$, which is equivalent to minimising $\frac{1}{2}||\mathbf{w}||^2$. This is convenient from an optimisation formulation perspective.

So, we can formulate the problem of finding $\mathbf{w}$ and $b$ as the optimisation problem:

\begin{equation}
min\frac{1}{2}||\mathbf{w}||^2\text{, such that }y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1 \geq 0\text{ }\forall{i}
\end{equation}

This is an optimisation problem with a convex quadratic objective function and linear constraints.

\subsubsection{Primal Problem}

The Lagrangian of the optimisation problem includes in equation (6) each constraint with it's associated multiplier ($\alpha_i \geq 0, \forall i$):

\begin{equation}
min\mathcal{L}_P(\mathbf{w}, b, \alpha) = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^{L}\alpha_i[y_i(\mathbf{x}_i\cdot\mathbf{w} + b) - 1]
\end{equation}

\begin{equation}
min\mathcal{L}_P(\mathbf{w}, b, \alpha) = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^{L}\alpha_i y_i(\mathbf{x}_i\cdot\mathbf{w} + b) + \sum_{i=1}^{L}\alpha_i
\end{equation}

Setting the derivatives of $\mathcal{L}_P$ with respect to $\mathbf{w}$ and $b$ to zero:

\begin{equation}
\frac{\partial \mathcal{L}_P}{\partial \mathbf{w}} = \mathbf{w} -  \sum_{i=1}^{L}\alpha_i y_i \mathbf{x}_i = 0
\end{equation}
 
\begin{equation}
\frac{\partial \mathcal{L}_P}{\partial b} = \sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

which yields:

\begin{equation}
\mathbf{w} = \sum_{i=1}^{L}\alpha_i y_i \mathbf{x}_i
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}

The Lagrangian is called the \emph{primal} form of the optimisation problem, or \emph{primal problem}, indicated by the $P$ subscript, $\mathcal{L}_P$.

\subsubsection{Dual Problem}

Whereas the \emph{primal problem} seeks to minimises $\mathcal{L}_P(\mathbf{w}, b, \alpha)$, the \emph{dual problem} seeks to maximise $\mathcal{L}_D(\alpha)$, where $\alpha$ is the $\emph{dual}$ variable[5], using the previously determined relations of equations (11) and (12).

Substituting (11) and (12) into (8) yields:

\begin{equation}
max\mathcal{L}_D(\alpha) = \sum_{i=1}^{L}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j - b\sum_{i=1}^{L}\alpha_i y_i
\end{equation}

From (12), the final term is zero, so the \emph{dual problem} simplifies to:

\begin{equation}
max\mathcal{L}_D(\alpha) = \sum_{i=1}^{L}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{L}\alpha_i \alpha_j y_i y_j\mathbf{x}_i\cdot\mathbf{x}_j
\end{equation}

such that:

\begin{equation}
\alpha_i\geq0\text{ }\forall{i}
\end{equation}

\begin{equation}
\sum_{i=1}^{L}\alpha_i y_i = 0
\end{equation}


So, what have we achieved in going from the \emph{primal problem} to the \emph{dual problem}? We have removed the dependency on $\mathbf{w}$ and $b$ from $\mathcal{L}_D(\alpha)$. We can now maximise $\mathcal{L}_D(\alpha)$, subject to the constraints, and then plug the resultant $\alpha$ into (11) to find $\mathbf{w}$. And subsequenty find $b$.

Importantly, the \emph{dual problem} is only dependent upon $y_i$ and the dot products $\mathbf{x}_i\cdot\mathbf{x}_j$. This can be exploited when considering non-linear classification.

\subsubsection{Hard Margin}

\subsubsection{Soft Margin}

\subsection{Non-Linear Classification}




\section{Simulation Study}

The subject of this project is to consider two different kernels for non-linear classification

\subsection{Linear Binary Classification}


\subsubsection{Chosen Data Set}


\subsubsection{Resulting Optimisation Problem}


\subsubsection{Solution of Optimisation Problem}


\subsubsection{Algorithm 1}


\subsubsection{Algorithm 2}


\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_data_0.pdf}
		\caption{Image 1}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_0.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_0.pdf}
		\caption{Image 1}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_0.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_data_1.pdf}
		\caption{Image 1}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_problem_dual_1.pdf}
		\caption{Image 1}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_2.pdf}
		\caption{Image 1}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{linear_svm_qp_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_2_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM - Randomly generated 2 points}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{linear_svm_rand_qp_3_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Linear SVM - Randomly generated 3 points}
\end{figure}





\subsection{Non-Linear Binary Classification}


\subsubsection{Chosen Data Set}


\subsubsection{Resulting Optimisation Problem}


\subsubsection{Solution of Optimisation Problem}


\subsubsection{Algorithm 1}


\subsubsection{Algorithm 2}


\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_qp_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_4_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM - Randomly generated 4 points}
\end{figure}

\begin{figure}[H]
	\centering	
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_0.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_1.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_2.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_3.pdf}
		\caption{image 2}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_4.pdf}
		\caption{Image 1}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{non_linear_svm_rand_qp_8_5.pdf}
		\caption{image 2}
	\end{subfigure}
	\caption{Non-Linear SVM - Randomly generated 8 points}
\end{figure}

\section{References}

[1] C. Cortes, V. Vapnik, in Machine Learning, pp. 273-297 (1995).

[2] T. Fletcher, Support Vector Machines Explained, March 1, 2009.

[3] A. Ng, CS229 Lecture Notes Part V, Support Vector Machines.

[4] A. G\'{e}ron, Hands-On Machine Learning with Scikit-Learn, Keras \& TensorFlow, Second Edition.

[5] R. Berwick, An Idiot's Guide to Support Vector Machines.

[6] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.


\section{Notes on Project Code}

The code for this project has been implemented in Python. For clarity, particularly for those readers with a MatLab background, is it worth highlighting noteworthy aspects of the coding.

\subsection{Lambda Expressions}
In Python functions are first class citizens, which means just they can be passed to/from functions in the same manner as variables. This is similar to MatLab function handles. It is sometimes convenient, and clearer, to write a short function as \emph{lambda} expressions, or \emph{anonymous functions}, rather that a write a full function declaration. A \emph{lambda} can also be assigned to a variable.

For example, the following function:

\begin{lstlisting}[language=Python]
	def linear_kernel(xi, xj):
		# @ is the dot product operator.
    		return xi @ xj
\end{lstlisting}

can be expressed as the following \emph{lambda} expression:

\begin{lstlisting}[language=Python]
	linear_kernel = lambda xi, xj: xi @ xj
\end{lstlisting}

I have used this functionality throughout my code for clarity and brevity.

For example, the \emph{dual problem} equation (as a minimisation):

\begin{equation}
\mathcal{L}_D(\alpha) = \frac{1}{2}\alpha^T\mathbf{H}\alpha - \sum_{i=1}^{L}\alpha_i
\end{equation}

is coded as follows (together with the first and second derivatives):

\begin{lstlisting}[language=Python]
    f   = lambda x: 0.5 * x @ H @ x - x @ (x / x)
    df  = lambda x: H @ x - x / x
    d2f = lambda x: H 
\end{lstlisting}

\subsection{Reimplementation of MatLab Functions}

%A number of MatLab functions that were used in previous COMP0120 assignments have been reimplemented in MatLab, specifically backtracking(), steepest_descent() and newton() functions.

A number of MatLab functions used in previous COMP0120 assignments have been reimplemented in Python, specifically $backtracking()$, $steepest\_descent()$ and $newton()$. The functionality of these functions was tested against the \emph{Rosenbrock} function and the results compared to the \emph{MatLab} results from Assignment 1.

\begin{figure}[H]
	\centering	
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{rosenbrock_0.pdf}
		\caption{$x0 = [1.2, 1.2]^T$}
	\end{subfigure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=0.75\textwidth]{rosenbrock_1.pdf}
		\caption{$x0 = [-1.2, 1.0]^T$}
	\end{subfigure}
	\caption{Functionality test of $backtracking()$, $steepest\_descent()$ and $newton()$ functions implemented in Python. With a value of $rho = 0.1$, both $steepest\_descent()$ and $newton()$ converge to $[1.0, 1.0]^T$ in the same number of iterations, and taking the same paths, as their \emph{MatLab} equivalent functions.}
\end{figure}


\end{document}


